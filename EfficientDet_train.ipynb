{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cTHDyODENsgr"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 및 모듈 import\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'NUM_CLASS':34,\n",
    "    'EPOCHS':30,\n",
    "    'ACCUMULATE':4,\n",
    "    'LR':3e-4,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':41\n",
    "}\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SENfg9eZNsgu"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image = np.array(Image.open(os.path.join(self.data_dir, image_info['file_name'])).convert('RGB'))\n",
    "        image = A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3, p=0.8)(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "        if self.transforms:\n",
    "            while True:\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels+1\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    target['labels'] = torch.tensor(sample['labels'])\n",
    "                    break\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W8FQbYrjNsgx"
   },
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        #A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3, p=0.8),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Kc0uStKpNsgy"
   },
   "outputs": [],
   "source": [
    "# loss 추적\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kGjDutKiNsgy"
   },
   "outputs": [],
   "source": [
    "# https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/config/model_config.py\n",
    "def get_net(checkpoint_path=None):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d0')\n",
    "    config.num_classes = CFG['NUM_CLASS']\n",
    "    config.image_size = (512,512)\n",
    "    \n",
    "    config.soft_nms = False\n",
    "    config.max_det_per_image = 25\n",
    "    \n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes) \n",
    "    \n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return DetBenchTrain(net)\n",
    "    \n",
    "# train function\n",
    "def train_fn(num_epochs, train_loader, optimizer, scheduler, model, device, clip=35):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        with tqdm(train_loader, unit = 'batch') as tepoch:\n",
    "            for images, targets, _ in tepoch:\n",
    "                tepoch.set_description(f'epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "                images = torch.stack(images) # bs, ch, w, h - 16, 3, 512, 512\n",
    "                images = images.to(device).float()\n",
    "                boxes = [target['boxes'].to(device).float() for target in targets]\n",
    "                labels = [target['labels'].to(device).float() for target in targets]\n",
    "                target = {\"bbox\": boxes, \"cls\": labels}\n",
    "\n",
    "                # calculate loss\n",
    "                loss, cls_loss, box_loss = model(images, target).values()\n",
    "                \n",
    "                # backward\n",
    "                (loss / CFG['ACCUMULATE']).backward()\n",
    "                \n",
    "                step += 1\n",
    "                if step % CFG['ACCUMULATE'] : \n",
    "                    continue\n",
    "                # grad clip\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                tepoch.set_postfix({'LR':round(scheduler.get_lr()[0],6),'loss':float(loss.detach().cpu()), 'loss_bbox':float(box_loss.detach().cpu()), 'loss_cls':float(cls_loss.detach().cpu())})\n",
    "            \n",
    "            torch.save(model.state_dict(), f'./ckp/epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OS1UR7dqNsgz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "annotation = './dataset/train.json'\n",
    "data_dir = './dataset/train'\n",
    "train_dataset = CustomDataset(annotation, data_dir, train_transform())\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "model = get_net()\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=CFG['LR'])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, threshold_mode='abs', min_lr=1e-6, verbose=True)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, eta_min=1e-6, T_0=405, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "# for images, targets, _ in tqdm(train_data_loader):\n",
    "#     labels += [target['labels'].tolist() for target in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# output = list(itertools.chain(*labels))\n",
    "# np.unique(output,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/30: 100%|██████████| 811/811 [15:25<00:00,  1.14s/batch, LR=0.000151, loss=1.16e+3, loss_bbox=0.00504, loss_cls=1.16e+3]\n",
      "epoch 2/30: 100%|██████████| 811/811 [14:23<00:00,  1.06s/batch, LR=0.0003, loss=636, loss_bbox=0.00534, loss_cls=635]         \n",
      "epoch 3/30: 100%|██████████| 811/811 [10:30<00:00,  1.29batch/s, LR=0.000256, loss=4.65, loss_bbox=0.00202, loss_cls=4.55]\n",
      "epoch 4/30: 100%|██████████| 811/811 [10:34<00:00,  1.28batch/s, LR=0.00015, loss=1.74, loss_bbox=0.000885, loss_cls=1.7]  \n",
      "epoch 5/30: 100%|██████████| 811/811 [10:32<00:00,  1.28batch/s, LR=4.5e-5, loss=1.53, loss_bbox=0.000527, loss_cls=1.5]   \n",
      "epoch 6/30: 100%|██████████| 811/811 [11:57<00:00,  1.13batch/s, LR=0.0003, loss=1.69, loss_bbox=0.000446, loss_cls=1.67]\n",
      "epoch 7/30: 100%|██████████| 811/811 [14:46<00:00,  1.09s/batch, LR=0.000288, loss=1.02, loss_bbox=0.000222, loss_cls=1.01]\n",
      "epoch 8/30: 100%|██████████| 811/811 [09:49<00:00,  1.37batch/s, LR=0.000256, loss=0.908, loss_bbox=0.000315, loss_cls=0.893]\n",
      "epoch 9/30: 100%|██████████| 811/811 [14:06<00:00,  1.04s/batch, LR=0.000207, loss=0.855, loss_bbox=0.000209, loss_cls=0.845]\n",
      "epoch 10/30: 100%|██████████| 811/811 [13:40<00:00,  1.01s/batch, LR=0.00015, loss=0.839, loss_bbox=0.000148, loss_cls=0.831] \n",
      "epoch 11/30: 100%|██████████| 811/811 [10:37<00:00,  1.27batch/s, LR=9.3e-5, loss=0.77, loss_bbox=0.000157, loss_cls=0.762]   \n",
      "epoch 12/30: 100%|██████████| 811/811 [10:34<00:00,  1.28batch/s, LR=4.4e-5, loss=0.759, loss_bbox=0.000268, loss_cls=0.746]\n",
      "epoch 13/30: 100%|██████████| 811/811 [10:33<00:00,  1.28batch/s, LR=1.2e-5, loss=0.741, loss_bbox=0.000172, loss_cls=0.732]\n",
      "epoch 14/30: 100%|██████████| 811/811 [09:50<00:00,  1.37batch/s, LR=0.0003, loss=0.738, loss_bbox=0.00016, loss_cls=0.73]  \n",
      "epoch 15/30: 100%|██████████| 811/811 [09:35<00:00,  1.41batch/s, LR=0.000297, loss=0.627, loss_bbox=0.000156, loss_cls=0.619]\n",
      "epoch 16/30: 100%|██████████| 811/811 [16:15<00:00,  1.20s/batch, LR=0.000288, loss=0.688, loss_bbox=0.000184, loss_cls=0.679]  \n",
      "epoch 17/30: 100%|██████████| 811/811 [10:44<00:00,  1.26batch/s, LR=0.000275, loss=0.393, loss_bbox=0.000186, loss_cls=0.383]\n",
      "epoch 18/30: 100%|██████████| 811/811 [10:36<00:00,  1.27batch/s, LR=0.000256, loss=0.298, loss_bbox=0.000154, loss_cls=0.29] \n",
      "epoch 19/30: 100%|██████████| 811/811 [10:25<00:00,  1.30batch/s, LR=0.000233, loss=0.225, loss_bbox=0.000113, loss_cls=0.219]\n",
      "epoch 20/30: 100%|██████████| 811/811 [10:22<00:00,  1.30batch/s, LR=0.000207, loss=0.276, loss_bbox=0.000319, loss_cls=0.26] \n",
      "epoch 21/30: 100%|██████████| 811/811 [10:24<00:00,  1.30batch/s, LR=0.000179, loss=0.189, loss_bbox=0.000104, loss_cls=0.184]\n",
      "epoch 22/30: 100%|██████████| 811/811 [10:25<00:00,  1.30batch/s, LR=0.00015, loss=0.157, loss_bbox=7.65e-5, loss_cls=0.153]  \n",
      "epoch 23/30: 100%|██████████| 811/811 [10:21<00:00,  1.31batch/s, LR=0.000121, loss=0.136, loss_bbox=7.46e-5, loss_cls=0.133] \n",
      "epoch 24/30: 100%|██████████| 811/811 [10:33<00:00,  1.28batch/s, LR=9.2e-5, loss=0.215, loss_bbox=8.15e-5, loss_cls=0.211]   \n",
      "epoch 25/30: 100%|██████████| 811/811 [10:28<00:00,  1.29batch/s, LR=6.7e-5, loss=0.123, loss_bbox=7.07e-5, loss_cls=0.12]  \n",
      "epoch 26/30: 100%|██████████| 811/811 [10:20<00:00,  1.31batch/s, LR=4.4e-5, loss=0.134, loss_bbox=6.36e-5, loss_cls=0.131] \n",
      "epoch 27/30: 100%|██████████| 811/811 [10:13<00:00,  1.32batch/s, LR=2.6e-5, loss=0.137, loss_bbox=6.68e-5, loss_cls=0.133] \n",
      "epoch 28/30: 100%|██████████| 811/811 [10:21<00:00,  1.31batch/s, LR=1.2e-5, loss=0.147, loss_bbox=9.48e-5, loss_cls=0.142]\n",
      "epoch 29/30: 100%|██████████| 811/811 [10:25<00:00,  1.30batch/s, LR=4e-6, loss=0.109, loss_bbox=6.09e-5, loss_cls=0.106]   \n",
      "epoch 30/30: 100%|██████████| 811/811 [10:26<00:00,  1.29batch/s, LR=0.0003, loss=0.127, loss_bbox=7.52e-5, loss_cls=0.124]\n"
     ]
    }
   ],
   "source": [
    "train_fn(CFG['EPOCHS'], train_data_loader, optimizer, scheduler, model, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import DetBenchPredict\n",
    "import gc\n",
    "\n",
    "# Effdet config를 통해 모델 불러오기 + ckpt load\n",
    "def load_net(checkpoint_path, device):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d0')\n",
    "    config.num_classes = 34\n",
    "    config.image_size = (512,512)\n",
    "    \n",
    "    config.soft_nms = False\n",
    "    config.max_det_per_image = 25\n",
    "    \n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "    net = DetBenchPredict(net)\n",
    "    net.load_state_dict(checkpoint)\n",
    "    net.eval()\n",
    "\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_box(box):\n",
    "    x1 = list(box)[0] * (1920/512)\n",
    "    y1 = list(box)[1] * (1080/512)\n",
    "    x2 = list(box)[2] * (1920/512)\n",
    "    y2 = list(box)[3] * (1080/512)\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nms\n",
    "from glob import glob\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, img_list, transform):\n",
    "        super().__init__()\n",
    "        self.img_list = img_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.img_list[idx]\n",
    "        img = Image.open(file_name).convert('RGB')\n",
    "        #img_size = torch.tensor(np.array(img).shape[:-1]).unsqueeze(0)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "        img = self.transform(image=np.array(img))['image']\n",
    "        return file_name, img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "# Albumentation을 이용, augmentation 선언\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "img_list = glob('./dataset/train/*.png')\n",
    "valid_dataset = ValidDataset(img_list, get_valid_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckp/epoch_30.pth'\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = load_net(checkpoint_path, device)\n",
    "model = model.to(device)\n",
    "\n",
    "file_name, image = valid_dataset[0]\n",
    "image = image.to(device).float()\n",
    "with torch.no_grad():\n",
    "    output = model(image.unsqueeze(0))\n",
    "outputs = []\n",
    "for out in output:\n",
    "    outputs.append({'boxes': out.detach().cpu().numpy()[:,:4], \n",
    "                    'scores': out.detach().cpu().numpy()[:,4], \n",
    "                    'labels': out.detach().cpu().numpy()[:,-1]})\n",
    "\n",
    "final_box = []\n",
    "final_score = []\n",
    "final_label = []\n",
    "for output in outputs:\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    labels = []\n",
    "    for box, score, label  in zip(output['boxes'],output['scores'],output['labels']):\n",
    "        x1, y1, x2, y2 = denormalize_box(box)\n",
    "        score = score\n",
    "        label = label\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "        scores.append(score)\n",
    "        labels.append(label)\n",
    "    picked_boxes, picked_score, picked_labels = nms(boxes, scores, labels, 0.5)\n",
    "\n",
    "    for box, score, label in zip(picked_boxes, picked_score, picked_labels):\n",
    "        if score < 0.5:\n",
    "            break\n",
    "        final_box.append(box)\n",
    "        final_score.append(score)\n",
    "        final_label.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "index = 1162\n",
    "img = Image.open(file_name)\n",
    "draw = ImageDraw.Draw(img, \"RGBA\")\n",
    "\n",
    "for i,j,k in zip(final_box, final_score, final_label):\n",
    "    draw.rectangle(tuple(i), outline='red', width=1)\n",
    "    draw.text((i[0],i[1]),text=str(k))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientDet_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
